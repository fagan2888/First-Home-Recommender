{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Home Recommender Wrangling and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the processes used to take the raw datafiles from the 2015 and 2017 American Housing Survey and transform them into datasets on which models can be built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect( \n",
    "                        host = ,\n",
    "                        port = , \n",
    "                        user = ,\n",
    "                        password = \n",
    "                        database = )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "DEC2FLOAT = psycopg2.extensions.new_type(psycopg2.extensions.DECIMAL.values,\n",
    "                                        'DEC2FLOAT',\n",
    "                                         lambda value, curs: float(value) if value is not None else None)\n",
    "psycopg2.extensions.register_type(DEC2FLOAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ds):\n",
    "    cursor.execute('Select * from \"{}\"'.format(ds))\n",
    "    rows = cursor.fetchall()\n",
    "    col_names = []\n",
    "    for elt in cursor.description:\n",
    "        col_names.append(elt[0])\n",
    "    return pd.DataFrame(data=rows, columns=col_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_2017 = create_dataset('_2017_household')\n",
    "mortgage_2017 = create_dataset('_2017_mortgage')\n",
    "person_2017 = create_dataset('_2017_person')\n",
    "project_2017 = create_dataset('_2017_project')\n",
    "household_2015 = create_dataset('_2015_household')\n",
    "mortgage_2015 = create_dataset('_2015_mortgage')\n",
    "person_2015 = create_dataset('_2015_person')\n",
    "project_2015 = create_dataset('_2015_project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Out Single Quotes and Convert Variables to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(df):\n",
    "    cols = list(df.columns)\n",
    "    for col in cols:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].str.strip(\"'\").astype('int64')\n",
    "        else:\n",
    "            pass\n",
    "    print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int(household_2017)\n",
    "convert_to_int(mortgage_2017)\n",
    "convert_to_int(person_2017)\n",
    "convert_to_int(project_2017)\n",
    "convert_to_int(household_2015)\n",
    "convert_to_int(mortgage_2015)\n",
    "convert_to_int(person_2015)\n",
    "convert_to_int(project_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Year Value\n",
    "household_2017['YEAR'] = 2017\n",
    "mortgage_2017['YEAR'] = 2017\n",
    "person_2017['YEAR'] = 2017\n",
    "project_2017['YEAR'] = 2017\n",
    "household_2015['YEAR'] = 2015\n",
    "mortgage_2015['YEAR'] = 2015\n",
    "person_2015['YEAR'] = 2015\n",
    "project_2015['YEAR'] = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Household, Mortgage, Person, and Project Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_datasets(df1, df2):\n",
    "    return pd.concat([df1, df2], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_combined = pd.concat([household_2017, household_2015], join='inner', ignore_index=True)\n",
    "mortgage_combined = pd.concat([mortgage_2017, mortgage_2015], join='inner', ignore_index=True)\n",
    "person_combined = pd.concat([person_2017, person_2015], join='inner', ignore_index=True)\n",
    "project_combined = pd.concat([project_2017, project_2015], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Clean Datasets to PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = {\n",
    "             'household_2017':household_2017, \n",
    "             'mortgage_2017':mortgage_2017, \n",
    "             'person_2017':person_2017,\n",
    "             'project_2017':project_2017,\n",
    "             'household_2015':household_2015, \n",
    "             'mortgage_2015':mortgage_2015, \n",
    "             'person_2015':person_2015,\n",
    "             'project_2015':project_2015,\n",
    "             'household_combined':household_combined,\n",
    "             'mortgage_combined':mortgage_combined,\n",
    "             'person_combined':person_combined,\n",
    "             'project_combined':project_combined\n",
    "            }\n",
    "engine = create_engine('postgresql://postgres:Admin123@project.cgxhdwn5zb5t.us-east-1.rds.amazonaws.com:5432/postgres')\n",
    "\n",
    "for name, df in df_tables.items():\n",
    "    df.to_sql('{}'.format(name), engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Combined Household Table and Variable Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we begin to pair down our data. We keep only the combined household table from the initial cleaning steps and begin to filter rows and drop variables. \n",
    "\n",
    "We use a metadata concordance table to subset the variables by certain characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.20\n",
    "\n",
    "#cursor.execute('Select * from household_combined')\n",
    "#rows = cursor.fetchall()\n",
    "#col_names = []\n",
    "#for elt in cursor.description:\n",
    "#    col_names.append(elt[0])\n",
    "#df = pd.DataFrame(data=rows, columns=col_names )\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Michael\\Workspace\\First-Home-Recommender\\data\\working\\AHS Household Combined.csv\")\n",
    "varcon = pd.read_csv(r\"C:\\Users\\Michael\\Workspace\\First-Home-Recommender\\data\\concordance\\variable_concordance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the dataset to only first-time home buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fh = df[df['FIRSTHOME']==1].copy()\n",
    "varcon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of each variable type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = varcon.iloc[:,0]\n",
    "weights_and_flags = varcon[(varcon['Weight']==True) | (varcon['Flag']==True)]['Variable'].values\n",
    "non_exp = varcon[varcon['Not Experience']==True]['Variable'].values\n",
    "target_vars = list(varcon[varcon['Type']=='Target']['Variable'].values)\n",
    "cont_vars = list(varcon[varcon['Type']=='Continuous']['Variable'].values)\n",
    "cat_vars = list(varcon[varcon['Type']=='Categorical']['Variable'].values)\n",
    "binary_vars = list(varcon[varcon['Type']=='Binary']['Variable'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate dataset into target variable and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28804, 260)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fh2 = df_fh[list(set(all_vars).difference(set(weights_and_flags)))].copy()\n",
    "target = df_fh2[target_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove variables whose portion of missing values is above the threshhold\n",
    "\n",
    "We remove any variales whose percentage of values are missing is above a certain threshold (in this case, 20%).\n",
    "\n",
    "Note: This process was slightly different between continuous, categorical, and binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.replace([-9], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[cont_vars].isin([-9,-6]).sum(axis=0) / df_fh2[cont_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < missing_threshold]\n",
    "estimators_cont = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_cont.replace([-9,-6], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[cat_vars].isin([-9]).sum(axis=0) / df_fh2[cat_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < missing_threshold]\n",
    "estimators_cat = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_cat.replace([-9], np.nan, inplace=True)\n",
    "\n",
    "miss_percent = df_fh2[binary_vars].isin([-9,-6]).sum(axis=0) / df_fh2[binary_vars].count(axis=0)\n",
    "miss_percent_lt_thresh = miss_percent[miss_percent.iloc[:] < missing_threshold]\n",
    "estimators_binary = df_fh2[['CONTROL','YEAR'] + list(miss_percent_lt_thresh.index)].copy()\n",
    "estimators_binary.replace([-9,-6], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only variales that capture \"housing experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [target, estimators_cont, estimators_cat, estimators_binary]\n",
    "df_fh_all_vars = reduce(lambda left, right: pd.merge(left, right, how='inner', \n",
    "                                                   on=['CONTROL','YEAR']), dfs).dropna(how='any')\n",
    "\n",
    "cont_vars = list(set(estimators_cont.columns) & set(non_exp))\n",
    "cat_vars = list(set(estimators_cat.columns) & set(non_exp))\n",
    "binary_vars = list(set(estimators_binary.columns) & set(non_exp))\n",
    "\n",
    "estimators_cont = estimators_cont[['CONTROL','YEAR'] + list(set(estimators_cont.columns) & set(non_exp))]\n",
    "estimators_cat = estimators_cat[['CONTROL','YEAR'] + list(set(estimators_cat.columns) & set(non_exp))]\n",
    "estimators_binary = estimators_binary[['CONTROL','YEAR'] + list(set(estimators_binary.columns) & set(non_exp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values for all estimators\n",
    "\n",
    "Missing values for continuous variales were imputed using the median value among first-time home buyers. Missing values for categorical and binary variables were imputed using the most frequent occuring class in the specific variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variables\n",
    "imputer_target = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_target.fit(target)\n",
    "imputed_target = imputer_target.transform(target)\n",
    "target = pd.DataFrame(imputed_target, columns=target.columns)\n",
    "\n",
    "# Continuous Variables\n",
    "imputer_cont = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer_cont.fit(estimators_cont)\n",
    "imputed_cont = imputer_cont.transform(estimators_cont)\n",
    "estimators_cont = pd.DataFrame(imputed_cont, columns=estimators_cont.columns)\n",
    "\n",
    "# Categorical Variables\n",
    "imputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_cat.fit(estimators_cat)\n",
    "imputed_cat = imputer_cat.transform(estimators_cat)\n",
    "estimators_cat = pd.DataFrame(imputed_cat, columns=estimators_cat.columns)\n",
    "\n",
    "#Binary Variables\n",
    "imputer_binary = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_binary.fit(estimators_binary)\n",
    "imputed_binary = imputer_binary.transform(estimators_binary)\n",
    "estimators_binary = pd.DataFrame(imputed_binary, columns=estimators_binary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets Back Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_cat_dum = pd.get_dummies(estimators_cat, columns=cat_vars)\n",
    "estimators_binary_dum = pd.get_dummies(estimators_binary, columns=binary_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_reg = [target, estimators_cont, estimators_cat_dum, estimators_binary_dum]\n",
    "dfs_class = [target, estimators_cont, estimators_cat, estimators_binary]\n",
    "df_final_reg = reduce(lambda left, right: pd.merge(left, right, how='inner', on=['CONTROL','YEAR']), dfs_reg).dropna(how='any')\n",
    "df_final_class = reduce(lambda left, right: pd.merge(left, right, how='inner', on=['CONTROL','YEAR']), dfs_class).dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Transformations\n",
    "\n",
    "### Take the Natural Log of Income Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_reg['LN_HINCP'] = np.where(df_final_reg['HINCP'] > 1, np.log(df_final_reg['HINCP']), 0)\n",
    "df_final_reg['LN_FINCP'] = np.where(df_final_reg['FINCP'] > 1, np.log(df_final_reg['FINCP']), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a floor and ceiling to the income variables and put them in 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_class['HINCP_BIN'] = np.where(df_final_class['HINCP']>100000, 100000, df_final_class['HINCP'])\n",
    "df_final_class['FINCP_BIN'] = np.where(df_final_class['FINCP']>100000, 100000, df_final_class['FINCP'])\n",
    "df_final_class['HINCP_BIN'] = np.where(df_final_class['HINCP']<=0, 0, df_final_class['HINCP'])\n",
    "df_final_class['FINCP_BIN'] = np.where(df_final_class['FINCP']<=0, 0, df_final_class['FINCP'])\n",
    "df_final_class['HINCP_BIN'] = pd.cut(df_final_class['HINCP'], bins=np.linspace(0,100000,11), include_lowest=True)\n",
    "df_final_class['FINCP_BIN'] = pd.cut(df_final_class['FINCP'], bins=np.linspace(0,100000,11), include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Transformed Data to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = { 'ahs_household_class':df_final_class,\n",
    "             'ahs_household_reg':df_final_reg}\n",
    "engine = create_engine('')\n",
    "\n",
    "for name, df in df_tables.items():\n",
    "    df.to_sql('{}'.format(name), engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
